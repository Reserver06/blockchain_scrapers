{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d55b93",
   "metadata": {},
   "source": [
    "# **Blockchain Scrapers**\n",
    "### Goal:\n",
    "The goal of this project is to scrape relevant information from 2 blockchain related websites. I have chosen to focus scraping article related data from www.cryptoslate.com and company record data from www.glyph.social. These websites contain a nice amount of clean blockchain related data so they are ideal candidates for scraping. I will use the Scrapy Python framework because it provides an easy way to scrape large amounts of data in a clean and understandable manner. The data will be stored in a MongoDB database because of its relatively effortless intergration with Scrapy. It is also a NoSQL database which is ideal for unstructured data. \n",
    "### Strategy:\n",
    "The strategy implemented for extracting data from these websites is as follows:\n",
    "- Check the website's robots.txt file for any exposed sitemaps (They are easier to scrape).\n",
    "- Since both sites have open sitemaps I will implement Scrapy's SitemapSpider to easily gather relevant urls.\n",
    "- Extract all the relevant data from the urls with scrapy leveraging xpath.\n",
    "- Create a pipline that will store all items in a MongoDB Database.\n",
    "- Have the raw html of each page downloaded and stored locally in a pickle format.\n",
    "- Export the collections from the database into a csv file using mongoexport.\n",
    "- Use Pandas to read back the data gathered in the CSV files.\n",
    "\n",
    "### Requirements:\n",
    "- Python3\n",
    "- MongoDB 6.0 or higher\n",
    "- mongoexport\n",
    "\n",
    "### Installation:\n",
    "In the project directory:\n",
    "- `pip install -r requirements.txt`\n",
    "\n",
    "### Example Usage:\n",
    "1. In the terminal, make sure you are in the blockchain_scrapers directory.\n",
    "2. Startup your MongoDB database on localhost port 27017\n",
    "\n",
    "- Crawl cryptoslate: `scrapy crawl cryptoslate`\n",
    "- Crawl glyph.social: `scrapy crawl glyph_social`\n",
    "- Focus crawling by category: `scrapy crawl <spider name> -a category='<desired category>'` \n",
    "\n",
    "### Notes:\n",
    "- All output is sent to the terminal and stored in the mongoDB database. You can store the yielded items in a json file by adding `-o <file name>.json` at the end of the above commands.\n",
    "- A download delay of 3 seconds has been implemented for both scrapers so we don't overwhelm the sites we are scraping.\n",
    "- The scrapers were designed on Windows 11 and have not been tested on Linux based systems. However, they were writen with linux systems in mind so they should run fine on them.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf55b9dc13acfcbacbb477694459a9d5e8d73a549c21d7d0f3557ad4815a72f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
