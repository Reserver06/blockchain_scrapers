{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d55b93",
   "metadata": {},
   "source": [
    "# **Blockchain Scrapers**\n",
    "### Goal:\n",
    "The goal of this project is to scrape relevant information from 2 blockchain related websites. I have chosen to focus scraping article related data from www.cryptoslate.com and company record data from www.glyph.social. These websites contain a nice amount of clean blockchain related data so they are ideal candidates for scraping. I will use the Scrapy Python framework because it provides an easy way to scrape large amounts of data in a clean and comprehensible manner. The data will be stored in a MongoDB database because of its relatively effortless intergration with Scrapy. It is also a NoSQL database which is ideal for unstructured data.\n",
    "### Strategy:\n",
    "The strategy implemented for extracting data from these websites is as follows:\n",
    "- Check the website's robots.txt file for any exposed sitemaps (They are easier to scrape).\n",
    "- Since both sites have open sitemaps I will implement Scrapy's SitemapSpider to easily gather relevant urls.\n",
    "- Extract all the relevant data from the urls with scrapy and xpath.\n",
    "- Create a pipline that will store all items in a MongoDB Database.\n",
    "- Have the raw html of each page downloaded and stored locally in a pickle format together with the url it came from. This strategy should help keep things more organized in storage while also reducing the size of each file saved.\n",
    "- Export the collections from the database into a csv file using mongoexport.\n",
    "- Use Pandas to read back the data gathered in the CSV files.\n",
    "\n",
    "### Requirements:\n",
    "- Python3\n",
    "- MongoDB 6.0 or higher\n",
    "- mongoexport\n",
    "\n",
    "### Installation:\n",
    "In the project directory:\n",
    "- `pip install -r requirements.txt`\n",
    "\n",
    "### Example Usage:\n",
    "1. In the terminal, make sure you are in the blockchain_scrapers directory.\n",
    "2. Startup your MongoDB database on localhost port 27017\n",
    "\n",
    "- Crawl cryptoslate: `scrapy crawl cryptoslate`\n",
    "- Crawl glyph.social: `scrapy crawl glyph_social`\n",
    "- Focus crawling by category on cryptoslate: `scrapy crawl cryptoslate -a category='<desired category>'` \n",
    "- Focus crawling by company on glyphsocial: `scrapy crawl glyph_social -a company='<desired company>'` \n",
    "\n",
    "### Notes:\n",
    "- All output is sent to the terminal and stored in the mongoDB database. You can store the yielded items in a json file by adding `-o <file name>.json` at the end of the above commands.\n",
    "- A download delay of 3 seconds has been implemented for both scrapers so we don't overwhelm the sites we are scraping.\n",
    "- The scrapers were designed on Windows 11 and have not been tested on Linux based systems. However, they were writen with linux systems in mind so they should run fine on them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0363f73",
   "metadata": {},
   "source": [
    "Reading the data from the MongoDB database into a CSV file can be done easily by running the following command with mongoexport:\n",
    "\n",
    "```\n",
    "mongoexport --db=<database_name> --collection=<collection_name> --type=csv \n",
    "                --fields=<field_names comma seperated> --out=<file_name>.csv\n",
    "```\n",
    "\n",
    "For example, if we wanted to extract the relevant data from our glyphsocial database we would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed59a9c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!mongoexport --db=glyphsocial --collection=company_records --type=csv --fields=url,legal_name,category,headquarters,founding_year,founding_team,facebook,linkedin,twitter,website,description --out=output_glyphsocial.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c667f",
   "metadata": {},
   "source": [
    "After running this command we should see an output similar to this:\n",
    "```\n",
    "2022-08-29T12:39:39.212-0400    connected to: mongodb://localhost/\n",
    "2022-08-29T12:39:39.221-0400    exported 115 records\n",
    "```\n",
    "We can then find our generated csv in the directory from which the command was executed. I have included 2 sample csv files with data generated by the scrapers. We can verify the data in the csv by using the Pandas libray. Since there are many columns in the csv and many of them are lengthy, I will only be verifying the first few columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output_glyphsocial.csv',usecols=['url','legal_name','category','headquarters','founding_year'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv',usecols=['author','topic','time_published'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00964bcb",
   "metadata": {},
   "source": [
    "We can go further by analyzing the data in the csv files to generate meaningful insights about the data we have collected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output.csv',usecols=['topic'])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Topic Frequency Analysis')\n",
    "df['topic'].value_counts().plot(kind='bar',xlabel='topic',ylabel='frequency')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf55b9dc13acfcbacbb477694459a9d5e8d73a549c21d7d0f3557ad4815a72f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
